#!/usr/bin/env python3
"""
Test for vLLM patching for distributed weight updates.

This script tests that:
1. The vLLM patches are applied successfully
2. Engine creation works with patches
3. Distributed updater process is spawned (if enabled)

Run with: python -m psyche.vllm.test_patched_integration
"""

import sys
import torch
from pathlib import Path

# Add parent directory to path if needed
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


def test_patch_application():
    """Test that vLLM patches are applied"""
    print("=" * 60)
    print("Test 1: Patch Application")
    print("=" * 60)

    try:
        from psyche.vllm import vllm_patch

        print("‚úì vLLM patch module imported")

        # Check if patches were applied
        import vllm.v1.worker.gpu_worker

        runner_class = vllm.v1.worker.gpu_worker.GPUModelRunner
        print(f"‚úì GPUModelRunner class: {runner_class}")

        # Check if it's our patched version
        if "Patched" in str(runner_class):
            print("‚úì GPUModelRunner appears to be patched")
        else:
            print("‚ö† GPUModelRunner may not be patched (this might be okay)")

        print("\n‚úÖ Patch application test PASSED\n")
        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_engine_with_patches():
    """Test engine creation with patches applied"""
    print("=" * 60)
    print("Test 2: Engine Creation with Patches")
    print("=" * 60)

    try:
        from psyche.vllm.engine import UpdatableLLMEngine

        print("Creating engine with gpt2...")
        engine = UpdatableLLMEngine(
            model_name="gpt2",
            tensor_parallel_size=1,
            dtype="auto",
            max_model_len=512,
            gpu_memory_utilization=0.3,
        )

        print("‚úì Engine created successfully")
        print("‚úì Patches applied (distributed updater will spawn if enabled)")

        # Test basic inference
        print("Testing inference...")
        request_id = engine.add_request(
            "Hello, world!", {"temperature": 0.0, "max_tokens": 5}
        )
        print(f"‚úì Request added: {request_id}")

        outputs = engine.step()
        print(f"‚úì Step completed, got {len(outputs)} outputs")

        print("\n‚úÖ Engine creation test PASSED\n")
        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """Run all tests"""
    print("\n" + "=" * 60)
    print("vLLM Patching Integration Test Suite")
    print("=" * 60 + "\n")

    results = []

    # Test 1: Patch application
    results.append(("Patch Application", test_patch_application()))

    # Test 2: Engine creation
    results.append(("Engine Creation", test_engine_with_patches()))

    # Summary
    print("=" * 60)
    print("Test Summary")
    print("=" * 60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{name:30} {status}")

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\nüéâ All tests passed!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())
