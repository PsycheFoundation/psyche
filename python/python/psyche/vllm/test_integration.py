#!/usr/bin/env python3
"""
Simple integration test for vLLM with weight updates.

This script tests the Python implementation without needing Rust.
Run with: python -m psyche.vllm.test_integration

Prerequisites:
- vLLM must be installed: pip install vllm
- PyTorch must be installed
"""

import sys
import torch
from pathlib import Path

# Add parent directory to path if needed
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


def test_engine_basic():
    """Test basic engine functionality"""
    print("=" * 60)
    print("Test 1: Basic Engine Functionality")
    print("=" * 60)

    try:
        from psyche.vllm.engine import UpdatableLLMEngine
        from psyche.vllm.vllm_patch import get_shared_state_dict

        print("Creating engine with gpt2...")
        engine = UpdatableLLMEngine(
            model_name="gpt2",
            tensor_parallel_size=1,
            dtype="auto",
            max_model_len=512,
            gpu_memory_utilization=0.3,
        )

        print("‚úì Engine created successfully")

        # Check if patches worked
        if hasattr(engine, "_using_patched_mode") and engine._using_patched_mode:
            print("‚úì Psyche patches active: using RPC-based weight updates")
        else:
            shared_state_dict = get_shared_state_dict(worker_id=0)
            if shared_state_dict is not None:
                print(
                    f"‚úì Shared memory patches working: {len(shared_state_dict)} parameters registered"
                )
            elif engine.param_registry:
                print(f"‚úì Engine has access to {len(engine.param_registry)} parameters")
            else:
                print("‚ö† No weight update mechanism available")

        # Test get_model()
        print("Getting model...")
        model = engine.get_model()
        if model is not None:
            print(f"‚úì Model retrieved: {type(model).__name__}")
        else:
            print("‚ö† Model not directly accessible (expected in vLLM 0.11+)")

        # Test share_memory()
        print("Calling share_memory...")
        engine.share_memory()
        print("‚úì share_memory() completed")

        # Test inference
        print("Adding request...")
        request_id = engine.add_request(
            "Hello, world!", {"temperature": 0.0, "max_tokens": 5}
        )
        print(f"‚úì Request added: {request_id}")

        print("Running step...")
        outputs = engine.step()
        print(f"‚úì Step completed, got {len(outputs)} outputs")

        print("\n‚úÖ Basic engine test PASSED\n")
        return True

    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        print("Make sure vLLM is installed: pip install vllm")
        return False
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_transforms():
    """Test weight transformation functions"""
    print("=" * 60)
    print("Test 2: Weight Transforms")
    print("=" * 60)

    try:
        from psyche.vllm.transforms import (
            apply_qkv_fusion,
            apply_gate_up_fusion,
            permute_for_rotary,
        )

        print("Testing rotary permutation...")
        weight = torch.randn(4096, 4096)
        permuted = permute_for_rotary(weight, n_heads=32)
        assert permuted.shape == weight.shape
        print("‚úì Rotary permutation works")

        print("Testing QKV fusion...")
        q = torch.randn(4096, 4096)
        k = torch.randn(4096, 4096)
        v = torch.randn(4096, 4096)
        qkv = apply_qkv_fusion(q, k, v, n_heads=32)
        assert qkv.shape == (4096 * 3, 4096)
        print("‚úì QKV fusion works")

        print("Testing gate-up fusion...")
        gate = torch.randn(11008, 4096)
        up = torch.randn(11008, 4096)
        gate_up = apply_gate_up_fusion(gate, up)
        assert gate_up.shape == (11008 * 2, 4096)
        print("‚úì Gate-up fusion works")

        print("\n‚úÖ Transforms test PASSED\n")
        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_updater_mock():
    """Test weight updater class"""
    print("=" * 60)
    print("Test 3: Weight Updater Class")
    print("=" * 60)

    try:
        from psyche.vllm.weight_updater import PsycheWeightUpdater

        print("Creating PsycheWeightUpdater...")
        # Create a mock state_dict
        mock_state_dict = {
            "layer1.weight": torch.randn(100, 100),
            "layer2.weight": torch.randn(50, 50),
        }

        updater = PsycheWeightUpdater(
            state_dict=mock_state_dict,
            model_config=None,
            training_world_size=1,
            inference_world_size=1,
            inference_rank=0,
        )
        print("‚úì PsycheWeightUpdater created")

        print("Testing weight update...")
        new_weights = {"layer1.weight": torch.randn(100, 100) * 0.01}
        updater.update_from_dict(new_weights)
        print("‚úì Weight update applied")

        print("\n‚úÖ Updater test PASSED\n")
        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_weight_update_direct():
    """Test direct weight updates via shared memory"""
    print("=" * 60)
    print("Test 4: Direct Weight Update")
    print("=" * 60)

    try:
        from psyche.vllm.engine import UpdatableLLMEngine

        print("Creating engine...")
        engine = UpdatableLLMEngine(
            model_name="gpt2",
            tensor_parallel_size=1,
            dtype="auto",
            max_model_len=512,
            gpu_memory_utilization=0.3,
        )
        print("‚úì Engine created")

        # For patched mode, test with a real GPT-2 parameter
        if hasattr(engine, "_using_patched_mode") and engine._using_patched_mode:
            print("Testing RPC-based weight update with real parameter...")

            try:
                # First, get list of all available parameters
                results = engine.engine.collective_rpc("get_psyche_param_names")
                if results and results[0]:
                    param_names = results[0]
                    print(f"  Found {len(param_names)} parameters")
                    print(f"  First 5 params: {param_names[:5]}")

                    # Pick a small parameter for testing (layer norm is usually small)
                    test_param = None
                    for name in param_names:
                        if "ln" in name.lower() or "norm" in name.lower():
                            test_param = name
                            break

                    # If no layer norm found, just use the first parameter
                    if test_param is None and param_names:
                        test_param = param_names[0]

                    if test_param:
                        # Get info for this parameter
                        param_results = engine.engine.collective_rpc(
                            "get_psyche_param_info", args=(test_param,)
                        )
                        if param_results and param_results[0]:
                            param_info = param_results[0]
                            print(f"  Testing with parameter: {test_param}")
                            print(
                                f"  Shape: {param_info['shape']}, dtype: {param_info['dtype']}"
                            )

                            # Create a test weight with the correct shape
                            test_weight = torch.randn(*param_info["shape"]) * 0.001
                            test_update = {test_param: test_weight}

                            print(
                                f"  Updating {test_param} with shape {list(test_weight.shape)}"
                            )
                            engine.update_weights(test_update)
                            print("‚úì RPC weight update call completed successfully")
                        else:
                            print("‚ö† Could not get parameter info")
                    else:
                        print("‚ö† No parameters available")
                else:
                    print("‚ö† Could not get parameter names from model")

            except Exception as e:
                print(f"‚ö† RPC weight update failed: {e}")
                import traceback

                traceback.print_exc()
        elif engine.param_registry:
            # Get a parameter to test
            param_name = list(engine.param_registry.keys())[0]
            original_param = engine.param_registry[param_name].data.clone()

            print(f"Testing update of: {param_name}")
            print(f"  Shape: {original_param.shape}, dtype: {original_param.dtype}")

            # Create update
            delta = torch.randn_like(original_param) * 0.001
            new_weight = original_param + delta

            # Apply update
            print("Applying weight update...")
            engine.update_weights({param_name: new_weight})

            # Verify update
            updated_param = engine.param_registry[param_name].data
            diff = (updated_param - new_weight).abs().max().item()

            print(f"  Max difference from expected: {diff}")

            if diff < 1e-6:
                print("‚úì Weight update applied correctly")
            else:
                print(f"‚ö† Weight update may not have applied correctly (diff={diff})")
        else:
            print("‚ö† No weight update mechanism available, skipping test")
            print("\n‚ö†Ô∏è  Weight update test SKIPPED\n")
            return True

        print("\n‚úÖ Weight update test PASSED\n")
        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_manager():
    """Test VLLMWithUpdater manager (requires vLLM)"""
    print("=" * 60)
    print("Test 5: VLLMWithUpdater Manager")
    print("=" * 60)

    try:
        from psyche.vllm.manager import VLLMWithUpdater

        print("Creating VLLMWithUpdater in direct mode...")
        vllm = VLLMWithUpdater(
            model_name="gpt2",
            mode="direct",
            tensor_parallel_size=1,
            gpu_memory_utilization=0.3,
            max_model_len=512,
        )
        print("‚úì Manager created successfully")

        print("Getting engine...")
        engine = vllm.engine
        print(f"‚úì Engine retrieved: {type(engine).__name__}")

        print("Testing weight update...")
        # Get a real parameter from the model to test with
        if hasattr(engine, "_using_patched_mode") and engine._using_patched_mode:
            # RPC mode: get parameter info
            results = engine.engine.collective_rpc("get_psyche_param_names")
            if results and results[0]:
                param_names = results[0]
                # Pick a small parameter
                test_param = None
                for name in param_names:
                    if "ln" in name.lower() or "norm" in name.lower():
                        test_param = name
                        break
                if test_param is None and param_names:
                    test_param = param_names[0]

                if test_param:
                    param_results = engine.engine.collective_rpc(
                        "get_psyche_param_info", args=(test_param,)
                    )
                    if param_results and param_results[0]:
                        param_info = param_results[0]
                        weight_delta = {
                            test_param: torch.randn(*param_info["shape"]) * 0.001
                        }
                        print(f"  Testing update for {test_param}")
                    else:
                        print("‚ö† Could not get parameter info, skipping weight update")
                        weight_delta = {}
                else:
                    print("‚ö† No parameters available, skipping weight update")
                    weight_delta = {}
            else:
                print("‚ö† Could not get parameter names, skipping weight update")
                weight_delta = {}
        else:
            # Legacy mode: use hardcoded GPT-2 parameter
            weight_delta = {
                "transformer.h.0.attn.c_attn.weight": torch.randn(2304, 768) * 0.001
            }

        if weight_delta:
            vllm.update_weights(weight_delta)
            print("‚úì Weight update completed")
        else:
            print("‚ö† Skipped weight update test")

        print("Creating checkpoint...")
        vllm.checkpoint()
        print("‚úì Checkpoint created")

        print("Shutting down...")
        vllm.shutdown()
        print("‚úì Shutdown complete")

        print("\n‚úÖ Manager test PASSED\n")
        return True

    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        print("Make sure vLLM is installed: pip install vllm")
        return False
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """Run all tests"""
    print("\n" + "=" * 60)
    print("vLLM Integration Test Suite")
    print("=" * 60 + "\n")

    results = []

    # Test 1: Basic engine (requires vLLM)
    results.append(("Basic Engine", test_engine_basic()))

    # Test 2: Transforms (pure PyTorch)
    results.append(("Transforms", test_transforms()))

    # Test 3: Updater class (no vLLM needed)
    results.append(("Weight Updater Class", test_updater_mock()))

    # Test 4: Direct weight update (requires vLLM with patches)
    results.append(("Direct Weight Update", test_weight_update_direct()))

    # Test 5: Full manager (requires vLLM)
    results.append(("VLLMWithUpdater", test_manager()))

    # Summary
    print("=" * 60)
    print("Test Summary")
    print("=" * 60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{name:20} {status}")

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\nüéâ All tests passed!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())
