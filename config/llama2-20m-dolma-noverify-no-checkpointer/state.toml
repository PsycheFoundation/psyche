run_id = "llama2-20m-dolma"
run_state = "WaitingForMembers"
run_state_start_unix_timestamp = 0
warmup_time = 30
cooldown_time = 0
checkpointers = ["flcbpgkeaq4mvhuy6uv5tiarf77hxoy44zahhvaxzl3s4fezhknq"]
overlapped = false
rounds_per_epoch = 20
max_round_train_time = 300
round_witness_time = 2
round_apply_time = 4
rounds_head = 0
first_round = false
min_clients = 2
tick = 0
last_tick_unix_timestamp = 0
batches_per_round = 8
data_indicies_per_batch = 1
max_batches_per_client = 10000
verification_percent = 0
witness_nodes = 1
witness_quorum = 1
epoch = 0
step = 1
last_step_unix_timestamp = 0
clients = []
dropped_clients = []
epoch_start_data_index = 0
total_steps = 25000
overlapepd = false

[[rounds]]
height = 0
clients_len = 0
data_index = 0
random_seed = 0
tie_breaker_tasks = 0
witnesses = []
[[rounds]]
height = 0
clients_len = 0
data_index = 0
random_seed = 0
tie_breaker_tasks = 0
witnesses = []
[[rounds]]
height = 0
clients_len = 0
data_index = 0
random_seed = 0
tie_breaker_tasks = 0
witnesses = []
[[rounds]]
height = 0
clients_len = 0
data_index = 0
random_seed = 0
tie_breaker_tasks = 0
witnesses = []
[model.LLM]
architecture = "HfLlama"
data_type = "Pretraining"
max_seq_len = 2048
[model.LLM.data_location]
Server = "127.0.0.1:20001"
[model.LLM.checkpoint.Hub]
repo_id = "emozilla/llama2-20m-init"
[model.LLM.lr_schedule.Cosine]
base_lr = 4.0e-4
warmup_steps = 500
warmup_init_lr = 0.0
total_steps = 25000
final_lr = 4.0e-5
[model.LLM.optimizer.Distro]
compression_decay = 0.999
compression_chunk = 64
compression_topk = 8
