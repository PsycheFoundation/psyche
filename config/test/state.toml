run_id = "test"
run_state = "WaitingForMembers"
warmup_time = 5
cooldown_time = 5
checkpointers = []
rounds_per_epoch = 20
max_round_train_time = 5
round_witness_time = 2
round_apply_time = 4
min_clients = 2
batches_per_round = 4
data_indicies_per_batch = 1
verification_percent = 0
witness_nodes = 1
witness_quorum = 1
total_steps = 10
overlapped = false

[model.LLM]
architecture = "HfLlama"
data_type = "Pretraining"
max_seq_len = 512
checkpoint = "Dummy"
optimizer = "Dummy"
data_location = "Dummy"


[model.LLM.lr_schedule.Cosine]
base_lr = 4.0e-4
warmup_steps = 20
warmup_init_lr = 0.0
total_steps = 2000
final_lr = 4.0e-5
