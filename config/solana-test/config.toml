[config]
warmup_time = 15
cooldown_time = 30
epoch_time = 60
max_round_train_time = 15
round_witness_time = 1
min_clients = 1
init_min_clients = 1
verification_percent = 0
witness_nodes = 0
global_batch_size_start = 2048
global_batch_size_end = 2048
global_batch_size_warmup_tokens = 0
total_steps = 25000
waiting_for_members_extra_time = 3

[model.LLM]
architecture = "HfLlama"
data_type = "Pretraining"
max_seq_len = 2048
cold_start_warmup_steps = 0
checkpoint = { Hub = { repo_id = "emozilla/llama2-1.1b-gqa-init" } }
data_locations = [
  { Http = { location = { Gcp = { bucket_name = "nous-pretraining-public-us", filter_directory = "fineweb-edu-tokenized-llama2" } }, token_size_in_bytes = "TwoBytes", shuffle = "DontShuffle" } },
]

[model.LLM.lr_schedule.Cosine]
base_lr = 4.0e-4
warmup_steps = 250
warmup_init_lr = 0.0
total_steps = 25000
final_lr = 4.0e-5

[model.LLM.optimizer.Distro]
clip_grad_norm = 1.0
compression_decay = 0.999
compression_chunk = 64
compression_topk = 2
quantize_1bit = false
