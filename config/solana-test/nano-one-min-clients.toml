[config]
warmup_time = 50
cooldown_time = 30
rounds_per_epoch = 20
max_round_train_time = 40
round_witness_time = 1
min_clients = 1
init_min_clients = 1
verification_percent = 0
witness_nodes = 1
global_batch_size_start = 4
global_batch_size_end = 4
global_batch_size_warmup_tokens = 0
total_steps = 25000

[model.LLM]
architecture = "HfLlama"
data_type = "Pretraining"
max_seq_len = 64
cold_start_warmup_steps = 0

[model.LLM.checkpoint.Hub]
repo_id = "pefontana/Nano-Llama"
revision = "cf48eac4944f6e954a3d9c9c30e8c865e64e7d03"

[model.LLM.data_location.Http]
token_size_in_bytes = "TwoBytes"
shuffle = "DontShuffle"

[model.LLM.data_location.Http.location]
SingleUrl = "https://huggingface.co/pefontana/Nano-Llama/resolve/main/tiny-ci-dataset/000_tiny-test.ds"

[model.LLM.lr_schedule.Cosine]
base_lr = 4.0e-4
warmup_steps = 250
warmup_init_lr = 0.0
total_steps = 25000
final_lr = 4.0e-5

[model.LLM.optimizer.Distro]
clip_grad_norm = 1.0
compression_decay = 0.999
compression_chunk = 64
compression_topk = 8
quantize_1bit = true
